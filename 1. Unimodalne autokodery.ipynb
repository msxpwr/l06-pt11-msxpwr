{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dcb60e",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66565ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0f1b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afebc7d",
   "metadata": {},
   "source": [
    "# 1. Unimodalne autokodery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c6389",
   "metadata": {},
   "source": [
    "Zanim przejdziemy do właściwych reprezentacji wielomodalnych sprawdzimy jak sprawdzają się reprezentacje unimodalne, tutaj: obrazowe oraz tekstowe.\n",
    "\n",
    "Z racji, że cechy obrazów pozyskaliśmy za pomocą sieci konwolucyjnej (ResNet50), a cechy tekstowe za pomocą modelu językowego (all-MiniLM-L6-v2), teraz będziemy wykorzystywać proste autokodery oparte o sieci MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.dataset import DataModule\n",
    "from src.downstream import evaluate_classification\n",
    "from src.nn.unimodal import UnimodalAE\n",
    "from src.train import extract_embeddings, train_model\n",
    "from src.visualization import make_interactive_scatter_plot, visualize_most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e56df",
   "metadata": {},
   "source": [
    "Określamy domyślne wartości hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hparams = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 30,\n",
    "    \"hidden_dims\": [256, 256, 256],\n",
    "    \"emb_dim\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c743517",
   "metadata": {},
   "source": [
    "Ładujemy dane dla uczenia modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(batch_size=default_hparams[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec8272",
   "metadata": {},
   "source": [
    "## Cechy wizualne\n",
    "Najpierw uczymy autokoder unimodalny, który będzie wykorzystywać tylko informacje o cechach pozyskanych z obrazka mema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=UnimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageAE\", \n",
    "        \"data_dim\": 2048, \n",
    "        \"modality_name\": \"img_emb\", \n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55d056",
   "metadata": {},
   "source": [
    "Wykorzystując wyuczony model (`name` definiuje, który model chcemy wykorzystać), zapisujemy wektory reprezentacji używając tylko części kodera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "unimodal_img_emb = extract_embeddings(\n",
    "    model_cls=UnimodalAE, \n",
    "    name=\"ImageAE\",\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c9c73",
   "metadata": {},
   "source": [
    "Wizualizujemy przestrzeń ukrytą:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Unimodal image embeddings\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(unimodal_img_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70908e32",
   "metadata": {},
   "source": [
    "Dla wybranej próbki danych znajdujemy najbliższe instancje w przestrzeni reprezentacji. Zachęcamy przejrzenia innych przykładów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade223c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_most_similar(\n",
    "    title=\"Most similar by image embedding\",\n",
    "    anchor_index=339,\n",
    "    z=unimodal_img_emb,\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0025b0c",
   "metadata": {},
   "source": [
    "## Cechy tekstowe\n",
    "Analogicznie wyuczymy model unimodalnego autokodera, który wykorzystuje tylko cechy pozyskane z napisów umieszczonych na memie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=UnimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"TextAE\",\n",
    "        \"data_dim\": 384,\n",
    "        \"modality_name\": \"text_emb\",\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "unimodal_text_emb = extract_embeddings(\n",
    "    model_cls=UnimodalAE,\n",
    "    name=\"TextAE\",\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Unimodal text embeddings\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(unimodal_text_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea12471",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_most_similar(\n",
    "    title=\"Most similar by text embedding\",\n",
    "    anchor_index=339,\n",
    "    z=unimodal_text_emb,\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83be65",
   "metadata": {},
   "source": [
    "## Ewaluacja wektorów reprezentacji\n",
    "Dotychczas ocena jakości reprezentacji była wykonywana oceniając grupowanie się przykładów w 2-wymiarowym rzucie przestrzeni reprezentacji oraz przeglądając podobne instancje. Wykorzystywany zbiór danych posiada również 5 niezależnych etykiet (`humour`, `motivational`, `offensive`, `sarcasm`, `sentiment`). Sprawdźmy zatem jak reprezentacje sprawdzają się w tych 5 zadaniach klasyfikacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef29191",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification(\n",
    "    model_names=[(UnimodalAE, \"ImageAE\"), (UnimodalAE, \"TextAE\")],\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
