{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc06003",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd379f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050d5aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afebc7d",
   "metadata": {},
   "source": [
    "# 2. Wielomodalne autokodery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c6389",
   "metadata": {},
   "source": [
    "Przejdziemy teraz do implementacji modelu **wielomodalnego autokodera**. W przypadku rozważanych przez nas danych, autokoder ten będzie posiadać dwa wejścia oraz dwa wyjścia (wcześniej wyznaczone wektory cech obrazków oraz tekstów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Type\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from src.dataset import DataModule\n",
    "from src.downstream import evaluate_classification\n",
    "from src.nn.unimodal import UnimodalAE\n",
    "from src.train import extract_embeddings, train_model\n",
    "from src.visualization import make_interactive_scatter_plot, visualize_most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9447",
   "metadata": {},
   "source": [
    "## Zadanie 2.1 (2 pkt)\n",
    "Zaczniemy od implementacji modułu kodera wielomodalnego. Należy uzupełnić poniższą implementację w taki sposób, aby:\n",
    "- dla każdej modalności (określonej przez parametr `modality_names`) został utworzony modal perceptrona wielowarstwowego (MLP), który będzie przekształcać cechy w danej modalności (pamiętaj aby odpowiednio przypisać moduły PyTorchowe – np. `ModuleList` albo `ModuleDict`)\n",
    "- MLP dla każdej modalności będzie posiadać taką samą architekturę (z wyłączeniem wymiaru wejściwego) - wykorzystaj podane w konstruktorze parametry dla tych sieci MLP:\n",
    "  * `in_dims` - wymiary danych wejściowych dla każdej modalności,\n",
    "  * `hidden_dims` - rozmiary warstw ukrytych, takie same dla każdego MLP,\n",
    "  * `out_dim` - wyjściowy rozmiar, również takie same dla każdego MLP.\n",
    "- w metodzie `forward()` przekształć odpowiednie modalności przez przypisane do nich sieci MLP, na wyjściu zwróć listę wektorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774a0bd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6b1ad700b6f5cda370b84dc976011d3",
     "grade": true,
     "grade_id": "multimodal-encoder",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultimodalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_names: List[str],\n",
    "        in_dims: Dict[str, int],\n",
    "        hidden_dims: List[int],\n",
    "        out_dim: int,\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_names = modality_names\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_hparams(hparams):\n",
    "        return MultimodalEncoder(\n",
    "            modality_names=hparams[\"modality_names\"],\n",
    "            in_dims=hparams[\"data_dims\"],\n",
    "            hidden_dims=hparams[\"hidden_dims\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "            last_activation=nn.Tanh,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618ff6f",
   "metadata": {},
   "source": [
    "## Zadanie 2.2 (2 pkt)\n",
    "Zaimplementuj dwie strategie łączenia wektorów ukrytych z różnych modalności w jeden wielomodalny wektor reprezentacji:\n",
    "- w klasie `AvgFusion` zaimplementuj uśrednianie wektorów z różnych modalności\n",
    "- w klasie `MLPFusion` skonkatenuj wektory z różnych modalności a następnie przekształć wynik przez sieć MLP (parametry sieci podane w konstruktorze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48263ec8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2259dbcd205b2d05d66c3f97381aec6",
     "grade": true,
     "grade_id": "fusion",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class AvgFusion(nn.Module):\n",
    "    \n",
    "    def forward(self, h: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "class MLPFusion(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_dim: int,\n",
    "        num_modalities: int,\n",
    "        hidden_dims: List[int],\n",
    "        out_dim: int,\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, h: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b23e96",
   "metadata": {},
   "source": [
    "## Zadanie 2.3 (2 pkt)\n",
    "Analogicznie do kodera wielomodalnego, musimy zaimplementować moduł wielomodalnego dekodera.\n",
    "- dla każdej modalności utwórz sieć MLP, która będzie dekodować (rekonstruować) oryginalne atrybuty obiektu w danej modalności:\n",
    "  * `in_dim` określa wymiar wejściowego wielomodalnego wektora reprezentacji (wspólne dla wszystkich modalności)\n",
    "  * `hidden_dims` określa rozmiary warstw ukrytych modeli MLP (wspólne dla wszystkich modalności)\n",
    "  * `out_dims` określa wymiary atrybutów (które chcemy zrekonstruować) w każdej modalności\n",
    "- w metodzie `forward()` zastosuj utworzone sieci MLP na wielomodalnej reprezentacji `z` i zwróć słownik, w którym klucze określają nazwy modalności a skojarzone wartości to rekonstrukcje atrybutów w danej modalności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79056cb1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a89ba6b741115fce33da645ec7df637",
     "grade": true,
     "grade_id": "multimodal-decoder",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MultimodalDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_names: List[str],\n",
    "        in_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        out_dims: Dict[str, int],\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_names = modality_names\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada28c2",
   "metadata": {},
   "source": [
    "## Zadanie 2.4 (2 pkt)\n",
    "Przeanalizuj implementację klasy bazowej `BaseAE` a następnie dokończ implementację właściwego wielomodalnego autokodera:\n",
    "- w metodzie `forward()` zastosuj wielomodalny koder `encoder` na podanych danych wejściowych, a następnie połącz listę ukrytych wektorów w jedną wielomodalną reprezentację, wykorzystując moduł fuzji `fusion`\n",
    "- w metodzie `_common_step()` zaimplementuj krok uczenia autokodera:\n",
    "  * wyznacz wielomodalną reprezentację `z`\n",
    "  * przeprowadź rekonstrukcję oryginalnych cech `x_rec` na podstawie reprezentacji `z`\n",
    "  * oblicz funkcję kosztu jako błąd średniokwadratowy (`MSE`) po każdej modalności, a wartości tych funkcji kosztu uśrednij względem wszystkich modalności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f50a2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4067690998126d2c7d37b3751de5cbf1",
     "grade": true,
     "grade_id": "multimodal-ae",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from src.nn.ae import BaseAE\n",
    "\n",
    "\n",
    "class MultimodalAE(BaseAE):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        encoder_cls = hparams[\"encoder_cls\"]\n",
    "        \n",
    "        super().__init__(\n",
    "            hparams=hparams,\n",
    "            encoder=encoder_cls.from_hparams(hparams),\n",
    "            decoder=MultimodalDecoder(\n",
    "                modality_names=hparams[\"modality_names\"],\n",
    "                in_dim=hparams[\"emb_dim\"],\n",
    "                hidden_dims=hparams[\"hidden_dims\"][::-1],\n",
    "                out_dims=hparams[\"data_dims\"],\n",
    "                last_activation=nn.Identity,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        if hparams[\"fusion\"] == \"Avg\":\n",
    "            self.fusion = AvgFusion()\n",
    "        elif hparams[\"fusion\"] == \"MLP\":\n",
    "            self.fusion = MLPFusion(\n",
    "                modality_dim=hparams[\"emb_dim\"],\n",
    "                num_modalities=len(hparams[\"modality_names\"]),\n",
    "                hidden_dims=[hparams[\"emb_dim\"], hparams[\"emb_dim\"]],\n",
    "                out_dim=hparams[\"emb_dim\"],\n",
    "                last_activation=nn.Tanh,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion module: \\\"{hparams['fusion']}\\\"\")\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _common_step(self, batch) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./data/logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hparams = {\n",
    "    \"encoder_cls\": MultimodalEncoder,\n",
    "    \"modality_names\": [\"img_emb\", \"text_emb\"],\n",
    "    \"data_dims\": {\"img_emb\": 2048, \"text_emb\": 384}, \n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 30,\n",
    "    \"hidden_dims\": [256, 256, 256],\n",
    "    \"emb_dim\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(batch_size=default_hparams[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageTextAvgAE\",\n",
    "        \"fusion\": \"Avg\",\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb004d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageTextMLPAE\",\n",
    "        \"fusion\": \"MLP\",\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_avg_emb = extract_embeddings(\n",
    "    model_cls=MultimodalAE, \n",
    "    name=\"ImageTextAvgAE\",\n",
    "    datamodule=datamodule,\n",
    ")\n",
    "\n",
    "multimodal_mlp_emb = extract_embeddings(\n",
    "    model_cls=MultimodalAE, \n",
    "    name=\"ImageTextMLPAE\",\n",
    "    datamodule=datamodule,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Multimodal embeddings (Avg)\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(multimodal_avg_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e019ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Multimodal embeddings (MLP)\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(multimodal_mlp_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade223c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_most_similar(\n",
    "    title=\"Most similar by multimodal embedding (Avg)\",\n",
    "    anchor_index=339,\n",
    "    z=multimodal_avg_emb,\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_most_similar(\n",
    "    title=\"Most similar by multimodal embedding (MLP)\",\n",
    "    anchor_index=339,\n",
    "    z=multimodal_mlp_emb,\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification(\n",
    "    model_names=[\n",
    "        (UnimodalAE, \"ImageAE\"), \n",
    "        (UnimodalAE, \"TextAE\"), \n",
    "        (MultimodalAE, \"ImageTextAvgAE\"),\n",
    "        (MultimodalAE, \"ImageTextMLPAE\"),\n",
    "    ],\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89382d98",
   "metadata": {},
   "source": [
    "# Maskowane uczenie\n",
    "Dotychczas wielomodalny autokoder był uczony w taki sposób, że zarówno na wejściu jak i na wyjściu otrzymywał informacje o obrazku, jak i tekście. Teraz zobaczymy jak model się będzie zachowywać w sytuacji, kiedy jedna z modalności będzie **maskowana na wejściu** (można w ten sposób symulować sytuacje, gdy jedna z modalności nie jest dostępna – *brakująca wartość*). \n",
    "\n",
    "Zaczniemy od przygotowania nowej implementacji `MultimodalEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a4a2d",
   "metadata": {},
   "source": [
    "## Zadanie 2.5 (2 pkt)\n",
    "Uzupełnij poniższą implementację klasy `MaskedMultimodalEncoder`, która będzie maskować cechy z wybranej modalności z określonym prawdopodobieństwem `p_m`:\n",
    "\n",
    "- poprzez maskowanie danej modalności rozumiemy zastąpienie wektora cech w tej modalności dla konkretnego obiektu, wektorem składającym się z samych zer\n",
    "- to czy pojedynczy obiekt będzie poddawany maskowaniu określamy na podstawie prawdopodobieństwa $p_m \\in [0, 1]$\n",
    "- zakładamy, że maskowanie dotyczy tylko etapu uczenia, natomiast w trakcie inferencji używamy dostępnych cech bez jakiejkolwiek modyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47681d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4e62a128a6cf9674945cfa67f44e4b9",
     "grade": true,
     "grade_id": "masked-multimodal-encoder",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MaskedMultimodalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_names: List[str],\n",
    "        in_dims: Dict[str, int],\n",
    "        hidden_dims: List[int],\n",
    "        out_dim: int,\n",
    "        last_activation: Type[nn.Module],\n",
    "        masked_modality: str,\n",
    "        p_m: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_names = modality_names\n",
    "        \n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_hparams(hparams):\n",
    "        return MaskedMultimodalEncoder(\n",
    "            modality_names=hparams[\"modality_names\"],\n",
    "            in_dims=hparams[\"data_dims\"],\n",
    "            hidden_dims=hparams[\"hidden_dims\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "            last_activation=nn.Tanh,\n",
    "            masked_modality=hparams[\"masked_modality\"],\n",
    "            p_m=hparams[\"p_m\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b85ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        **default_hparams,\n",
    "        \"name\": \"MaskedImage_ImageTextAvgAE\",\n",
    "        \"fusion\": \"Avg\",\n",
    "        \"encoder_cls\": MaskedMultimodalEncoder,\n",
    "        \"masked_modality\": \"img_emb\",\n",
    "        \"p_m\": 1.0,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        **default_hparams,\n",
    "        \"name\": \"MaskedText_ImageTextAvgAE\",\n",
    "        \"fusion\": \"Avg\",\n",
    "        \"encoder_cls\": MaskedMultimodalEncoder,\n",
    "        \"masked_modality\": \"text_emb\",\n",
    "        \"p_m\": 1.0,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image_avg_emb = extract_embeddings(\n",
    "    model_cls=MultimodalAE, \n",
    "    name=\"MaskedImage_ImageTextAvgAE\",\n",
    "    datamodule=datamodule,\n",
    ")\n",
    "\n",
    "masked_text_avg_emb = extract_embeddings(\n",
    "    model_cls=MultimodalAE, \n",
    "    name=\"MaskedText_ImageTextAvgAE\",\n",
    "    datamodule=datamodule,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Masked Image (p_m = 1.0) - Multimodal embeddings (Avg)\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(masked_image_avg_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_interactive_scatter_plot(\n",
    "    title=\"Masked Text (p_m = 1.0) - Multimodal embeddings (Avg)\",\n",
    "    z_2d=PCA(n_components=2).fit_transform(masked_text_avg_emb),\n",
    "    df=datamodule.df[\"all\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66094350",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification(\n",
    "    model_names=[\n",
    "        (UnimodalAE, \"ImageAE\"), \n",
    "        (UnimodalAE, \"TextAE\"), \n",
    "        (MultimodalAE, \"ImageTextAvgAE\"),\n",
    "        (MultimodalAE, \"ImageTextMLPAE\"),\n",
    "        (MultimodalAE, \"MaskedImage_ImageTextAvgAE\"),\n",
    "        (MultimodalAE, \"MaskedText_ImageTextAvgAE\"),\n",
    "    ],\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e2105",
   "metadata": {},
   "source": [
    "## Zadanie 2.6 (2 pkt)\n",
    "Zbadaj jak wartość parametru `p_m` wpływa na jakość otrzymywanych multimodalnych wektorów reprezentacji? (Skrajne wartości `p_m = 0` oraz `p_m = 1` zbadaliśmy w poprzednich przykładach). Skomentuj otrzymane wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f3d4ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ecccebf53976578f549fb8e2b696d0f",
     "grade": true,
     "grade_id": "prob-study",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_masking_probability_performance():\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
